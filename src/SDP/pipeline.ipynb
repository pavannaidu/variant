{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26bc0222",
   "metadata": {},
   "source": [
    "# Spark Declarative Pipeline: from_json Schema Inference & Evolution\n",
    "\n",
    "This notebook demonstrates Databricks Spark Declarative Pipelines with `from_json` automatic schema inference and evolution.\n",
    "\n",
    "## Features Demonstrated:\n",
    "1. **Automatic Schema Inference** - `from_json` with `schemaLocationKey`\n",
    "2. **Schema Evolution** - Handling new fields automatically\n",
    "3. **Multiple Evolution Modes** - `addNewColumns`, `rescue`, `none`\n",
    "4. **Schema Hints** - Guiding type inference\n",
    "5. **Bronze ‚Üí Silver ‚Üí Gold** pattern with evolving schemas\n",
    "\n",
    "## References:\n",
    "- [from_json Schema Inference Documentation](https://docs.databricks.com/aws/en/ldp/from-json-schema-evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION PARAMETERS (Read from SDP pipeline configuration)\n",
    "# =============================================================================\n",
    "# These values are passed from the Databricks Asset Bundle configuration\n",
    "# and can be accessed via spark.conf.get() with defaults for local testing\n",
    "CATALOG = spark.conf.get(\"catalog\", \"pavan_naidu\")\n",
    "SCHEMA = spark.conf.get(\"schema\", \"json\")\n",
    "VOLUME = spark.conf.get(\"volume\", \"raw_data\")\n",
    "SOURCE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/users_stream/\"\n",
    "# =============================================================================\n",
    "\n",
    "print(\"‚úÖ Spark Declarative Pipeline Configuration\")\n",
    "print(f\"üìÇ Source: {SOURCE_PATH}\")\n",
    "print(f\"üóÑÔ∏è  Target: {CATALOG}.{SCHEMA}\")\n",
    "print(f\"üì¶ Volume: {VOLUME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece02d7",
   "metadata": {},
   "source": [
    "## Bronze Layer: Auto Schema Inference with from_json\n",
    "\n",
    "Reads JSON lines with automatic schema inference and evolution:\n",
    "- **cloudFiles format**: `text` (reads raw JSON as text)\n",
    "- **from_json schema**: `None` (automatic inference)\n",
    "- **schemaLocationKey**: Unique key to track inferred schema\n",
    "- **schemaEvolutionMode**: `addNewColumns` (automatically adds new fields as they appear)\n",
    "\n",
    "Three schema evolution strategies demonstrated:\n",
    "1. **addNewColumns** - New fields added to schema automatically\n",
    "2. **Schema Hints** - Guide type inference (e.g., `\"age INT, created_at TIMESTAMP\"`)\n",
    "3. **Rescue Mode** - New fields stored in separate `rescued_fields` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b278061",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(\n",
    "    comment=\"Bronze layer: Raw user data with automatic schema inference using from_json\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def users_bronze():\n",
    "    \"\"\"\n",
    "    Read JSON lines from cloud storage and parse with automatic schema inference using from_json.\n",
    "    The schema is inferred from the first batch and automatically evolves as new fields appear.\n",
    "    Data is kept as a struct column for schema evolution support.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"text\")  # Read as text to get 'value' column\n",
    "            .load(SOURCE_PATH)\n",
    "            .select(\n",
    "                col(\"_metadata.file_path\").alias(\"source_file\"),\n",
    "                col(\"_metadata.file_modification_time\").alias(\"ingestion_time\"),\n",
    "                from_json(\n",
    "                    col(\"value\"), \n",
    "                    None,  # NULL schema = automatic inference\n",
    "                    {\n",
    "                        \"schemaLocationKey\": \"users_bronze_json\",  # Unique key for this schema\n",
    "                        \"schemaEvolutionMode\": \"addNewColumns\"     # Add new fields automatically\n",
    "                    }\n",
    "                ).alias(\"data\"),\n",
    "                from_json(\n",
    "                    col(\"value\"), \n",
    "                    None,  # NULL schema = automatic inference\n",
    "                    {\n",
    "                        \"schemaLocationKey\": \"users_bronze_schema_hints\",  # Unique key for this schema\n",
    "                        \"schemaEvolutionMode\": \"addNewColumns\",     # Add new fields automatically\n",
    "                        # Schema hints: guide inference for specific fields\n",
    "                        \"schemaHints\": \"age INT, created_at TIMESTAMP\"\n",
    "                    }\n",
    "                ).alias(\"data_schema_hints\"),\n",
    "                from_json(\n",
    "                    col(\"value\"), \n",
    "                    None,  # NULL schema = automatic inference\n",
    "                    {\n",
    "                        \"schemaLocationKey\": \"users_bronze_rescue\", # Unique key for this schema\n",
    "                        \"schemaEvolutionMode\": \"rescue\",  # New fields ‚Üí _rescued_data\n",
    "                        \"rescuedDataColumn\": \"rescued_fields\"  # Custom name for rescued data\n",
    "                    }\n",
    "                ).alias(\"data_rescued\")\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145ac1a",
   "metadata": {},
   "source": [
    "## Silver Layer: Extract and Validate\n",
    "\n",
    "Extract commonly-used fields from the inferred JSON structure for optimized queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(\n",
    "    comment=\"Silver layer: Extract and flatten key fields with data quality checks\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "@dp.expect_or_drop(\"valid_age\", \"age IS NOT NULL AND age BETWEEN 0 AND 120\")\n",
    "@dp.expect_or_drop(\"valid_email\", \"email IS NOT NULL AND email LIKE '%@%'\")\n",
    "def users_silver():\n",
    "    \"\"\"\n",
    "    Flatten and extract commonly-accessed fields from the bronze layer data struct.\n",
    "    Only accesses fields available in Phase 1 data.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"users_bronze\")\n",
    "            .select(\n",
    "                # Top-level fields from data struct (Phase 1)\n",
    "                col(\"data.user_id\").alias(\"user_id\"),\n",
    "                col(\"data.username\").alias(\"username\"),\n",
    "                col(\"data.email\").alias(\"email\"),\n",
    "                col(\"data.name\").alias(\"name\"),\n",
    "                col(\"data.age\").cast(\"int\").alias(\"age\"),\n",
    "                col(\"data.phone\").alias(\"phone\"),\n",
    "                \n",
    "                # Nested address fields (Phase 1)\n",
    "                col(\"data.address.street\").alias(\"street\"),\n",
    "                col(\"data.address.city\").alias(\"city\"),\n",
    "                col(\"data.address.state\").alias(\"state\"),\n",
    "                col(\"data.address.country\").alias(\"country\"),\n",
    "                col(\"data.address.postal_code\").alias(\"postal_code\"),\n",
    "                col(\"data.address.coordinates.latitude\").cast(\"double\").alias(\"latitude\"),\n",
    "                col(\"data.address.coordinates.longitude\").cast(\"double\").alias(\"longitude\"),\n",
    "                \n",
    "                # Nested profile fields (Phase 1)\n",
    "                col(\"data.profile.bio\").alias(\"bio\"),\n",
    "                col(\"data.profile.occupation\").alias(\"occupation\"),\n",
    "                col(\"data.profile.company\").alias(\"company\"),\n",
    "                \n",
    "                # Optional referral fields (Phase 1)\n",
    "                col(\"data.referral_code\").alias(\"referral_code\"),\n",
    "                col(\"data.referred_by\").alias(\"referred_by\"),\n",
    "                \n",
    "                # Timestamps (Phase 1)\n",
    "                col(\"data.created_at\").cast(\"timestamp\").alias(\"created_at\"),\n",
    "                col(\"data.last_login\").cast(\"timestamp\").alias(\"last_login\"),\n",
    "                \n",
    "                # Metadata\n",
    "                col(\"source_file\"),\n",
    "                col(\"ingestion_time\"),\n",
    "                current_timestamp().alias(\"processed_time\")\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb0e72",
   "metadata": {},
   "source": [
    "## Gold Layer: Aggregations and Analytics\n",
    "\n",
    "Create business-ready aggregated tables for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(\n",
    "    comment=\"Gold layer: User statistics by occupation\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\"\n",
    "    }\n",
    ")\n",
    "def user_stats_by_occupation():\n",
    "    \"\"\"\n",
    "    Aggregate user statistics by occupation.\n",
    "    Uses only Phase 1 fields available in the initial data.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read.table(\"users_silver\")\n",
    "            .groupBy(\"occupation\")\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"user_count\"),\n",
    "                avg(\"age\").alias(\"avg_age\"),\n",
    "                countDistinct(\"country\").alias(\"unique_countries\"),\n",
    "                countDistinct(\"company\").alias(\"unique_companies\"),\n",
    "                min(\"created_at\").alias(\"first_user_created\"),\n",
    "                max(\"created_at\").alias(\"last_user_created\")\n",
    "            )\n",
    "            .orderBy(desc(\"user_count\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eaa68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(\n",
    "    comment=\"Gold layer: Geographic user distribution\"\n",
    ")\n",
    "def user_geographic_summary():\n",
    "    \"\"\"\n",
    "    Aggregate users by geographic location.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dp.read(\"users_silver\")\n",
    "            .groupBy(\"country\", \"state\")\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"user_count\"),\n",
    "                avg(\"age\").alias(\"avg_age\"),\n",
    "                countDistinct(\"city\").alias(\"unique_cities\")\n",
    "            )\n",
    "            .orderBy(desc(\"user_count\"))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
